{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CS535 Pytorch Tutorial - Define a Convolutional Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Construct Your Network\n",
    "In pytorch any neual network is a subclass that derived from its superclass ```torch.nn.Module```.\n",
    "Let's build a toy CNN network to see how this works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (conv1): Conv2d (1, 6, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d (6, 16, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (fc1): Linear(in_features=400, out_features=120)\n",
      "  (fc2): Linear(in_features=120, out_features=84)\n",
      "  (fc3): Linear(in_features=84, out_features=10)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class Net(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        # 1 input image channel, 6 output channels, 5x5 square convolution\n",
    "        # kernel\n",
    "        # torch.nn.Conv2d(in_channels, out_channels, kernel_size, stride=1, \n",
    "        #                padding=0, dilation=1, groups=1, bias=True)\n",
    "        self.conv1 = nn.Conv2d(1, 6, 5)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        # an affine operation: y = Wx + b\n",
    "        # torch.nn.Linear(in_features, out_features, bias=True)\n",
    "        self.fc1 = nn.Linear(16 * 5 * 5, 120) #32x32 -> 28x28 -> 14x14 -> 10x10 -> 5x5\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 10)\n",
    "        # Now we have defined the above components of our network, the next\n",
    "        # step is to connect them into the order we wanted\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Max pooling over a (2, 2) window\n",
    "        x = self.conv1(x)\n",
    "        x = F.relu(x)\n",
    "        x = F.max_pool2d(x, (2, 2))\n",
    "        # Or, you can put this into one line:\n",
    "        x = F.max_pool2d(F.relu(self.conv2(x)), 2)\n",
    "        # Tensor.view() == numpy.reshape()\n",
    "        x = x.view(-1, self.num_flat_features(x))\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "    def num_flat_features(self, x):\n",
    "        size = x.size()[1:]  # all dimensions except the batch dimension\n",
    "        num_features = 1\n",
    "        for s in size:\n",
    "            num_features *= s\n",
    "        return num_features\n",
    "\n",
    "\n",
    "net = Net()\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You just have to define the ```forward``` function, and the ```backward``` function (where gradients are computed) is automatically defined for you using autograd. You can use any of the Tensor operations in the forward function.\n",
    "\n",
    "The learnable parameters of a model are returned by ```net.parameters()```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      "[torch.Size([6, 1, 5, 5]), torch.Size([6]), torch.Size([16, 6, 5, 5]), torch.Size([16]), torch.Size([120, 400]), torch.Size([120]), torch.Size([84, 120]), torch.Size([84]), torch.Size([10, 84]), torch.Size([10])]\n"
     ]
    }
   ],
   "source": [
    "params = list(net.parameters())\n",
    "print(len(params))\n",
    "print([x.size() for x in params])  # weight and bias for each layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The input to the forward is an autograd.Variable, and so is the output. Note: Expected input size to this net(LeNet) is ```32x32```. To use this net on MNIST dataset,please resize the images from the dataset to ```32x32```.\n",
    "In Pytorch your input must be feeded in mini-batch, even if you want to feed your data one by one, you have to use  ```input.unsqueeze(0)``` to expand your input dimensionality.\n",
    "\n",
    "For 2D images, your input dimension should be: ```[batch_size, channel, height, width]```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable containing:\n",
      "-0.0249 -0.1065 -0.0304  0.1617 -0.0185 -0.0671  0.1201 -0.0193 -0.0295  0.0818\n",
      "[torch.FloatTensor of size 1x10]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "input = Variable(torch.randn(1, 1, 32, 32))\n",
    "out = net(input)\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Loss Function\n",
    "A loss function takes the (output, target) pair of inputs, and computes a value that estimates how far away the output is from the target.\n",
    "\n",
    "There are several different loss functions under the nn package . A simple loss is: ```nn.MSELoss``` which computes the mean-squared error between the input and the target."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable containing:\n",
      " 3\n",
      "[torch.LongTensor of size 1]\n",
      "\n",
      "(Variable containing:\n",
      "-0.0249 -0.1065 -0.0304  0.1617 -0.0185 -0.0671  0.1201 -0.0193 -0.0295  0.0818\n",
      "[torch.FloatTensor of size 1x10]\n",
      ", Variable containing:\n",
      " 3\n",
      "[torch.LongTensor of size 1]\n",
      ", Variable containing:\n",
      " 2.1510\n",
      "[torch.FloatTensor of size 1]\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "output = net(input)\n",
    "target = Variable(torch.Tensor([3]).type(torch.LongTensor))\n",
    "# a dummy target, for example\n",
    "print(target)\n",
    "# Log-Softmax-Crossentropy loss \n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "loss = criterion(output, target)\n",
    "print(output, target, loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, if you follow loss in the backward direction, using it’s .grad_fn attribute, you will see a graph of computations that looks like this:\n",
    "```\n",
    "input -> conv2d -> relu -> maxpool2d -> conv2d -> relu -> maxpool2d\n",
    "      -> view -> linear -> relu -> linear -> relu -> linear\n",
    "      -> CrossEntropyLoss\n",
    "      -> loss\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, when we call loss.backward(), the whole graph is differentiated w.r.t. the loss, and all Variables in the graph will have their .grad Variable accumulated with the gradient.\n",
    "\n",
    "For illustration, let us follow a few steps backward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<NllLossBackward object at 0x3863150>\n",
      "<LogSoftmaxBackward object at 0x3863350>\n",
      "<AddmmBackward object at 0x38634d0>\n"
     ]
    }
   ],
   "source": [
    "print(loss.grad_fn)  # CrossEntropyLoss\n",
    "print(loss.grad_fn.next_functions[0][0])  # Log Softmax loss\n",
    "print(loss.grad_fn.next_functions[0][0].next_functions[0][0])  # Linear"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Back Propagation\n",
    "To backpropagate the error all we have to do is to ```loss.backward()```. You need to clear the existing gradients though, else gradients will be accumulated to existing gradients\n",
    "\n",
    "Now we shall call ```loss.backward()```, and have a look at conv1’s bias gradients before and after the backward."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conv1.bias.grad before backward\n",
      "None\n",
      "conv1.bias.grad after backward\n",
      "Variable containing:\n",
      "1.00000e-03 *\n",
      " -4.2934\n",
      " -2.8786\n",
      " -3.2121\n",
      "  3.1474\n",
      " -6.7055\n",
      " -3.3181\n",
      "[torch.FloatTensor of size 6]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "net.zero_grad()     # zeroes the gradient buffers of all parameters\n",
    "\n",
    "print('conv1.bias.grad before backward')\n",
    "print(net.conv1.bias.grad)\n",
    "\n",
    "loss.backward()\n",
    "\n",
    "print('conv1.bias.grad after backward')\n",
    "print(net.conv1.bias.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Update the Weights\n",
    "The simplest update rule used in practice is the Stochastic Gradient Descent (SGD):\n",
    "\n",
    "```weight = weight - learning_rate * gradient```\n",
    "\n",
    "\n",
    "We can implement this using simple python code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.01\n",
    "for f in net.parameters():\n",
    "    f.data.sub_(f.grad.data * learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, as you use neural networks, you want to use various different update rules such as SGD, Nesterov-SGD, Adam, RMSProp, etc. To enable this, we built a small package: torch.optim that implements all these methods. Using it is very simple:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "# create your optimizer\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.01)\n",
    "\n",
    "# in your training loop:\n",
    "optimizer.zero_grad()   # zero the gradient buffers\n",
    "output = net(input)\n",
    "loss = criterion(output, target)\n",
    "loss.backward()\n",
    "optimizer.step()    # Does the update"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Pytorch Kernel",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
