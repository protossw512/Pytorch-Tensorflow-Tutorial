{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CS540 Pytorch Tutorial - Variable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Variable\n",
    "--------\n",
    "\n",
    "``autograd.Variable`` is the central class of the package. It wraps a\n",
    "Tensor, and supports nearly all of operations defined on it. Once you\n",
    "finish your computation you can call ``.backward()`` and have all the\n",
    "gradients computed automatically.\n",
    "\n",
    "You can access the raw tensor through the ``.data`` attribute, while the\n",
    "gradient w.r.t. this variable is accumulated into ``.grad``.\n",
    "\n",
    ".. figure:: /_static/img/Variable.png\n",
    "   :alt: Variable\n",
    "\n",
    "   Variable\n",
    "\n",
    "There’s one more class which is very important for autograd\n",
    "implementation - a ``Function``.\n",
    "\n",
    "``Variable`` and ``Function`` are interconnected and build up an acyclic\n",
    "graph, that encodes a complete history of computation. Each variable has\n",
    "a ``.grad_fn`` attribute that references a ``Function`` that has created\n",
    "the ``Variable`` (except for Variables created by the user - their\n",
    "``grad_fn is None``).\n",
    "\n",
    "If you want to compute the derivatives, you can call ``.backward()`` on\n",
    "a ``Variable``. If ``Variable`` is a scalar (i.e. it holds a one element\n",
    "data), you don’t need to specify any arguments to ``backward()``,\n",
    "however if it has more elements, you need to specify a ``grad_output``\n",
    "argument that is a tensor of matching shape.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a varible:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable containing:\n",
      " 1\n",
      "[torch.FloatTensor of size 1x1]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "x = Variable(torch.ones(1,1), requires_grad=True)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable containing:\n",
      " 5\n",
      "[torch.FloatTensor of size 1x1]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y = 2 * x + 3\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable containing:\n",
      " 75\n",
      "[torch.FloatTensor of size 1x1]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "z = y * y * 3\n",
    "\n",
    "print(z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To reduce memory usage, during the .backward() call, all the intermediary results are deleted when they are not needed anymore. Hence if you try to call .backward() again, the intermediary results don’t exist and the backward pass cannot be performed (and you get the error you see).\n",
    "You can call .backward(retain_graph=True) to make a backward pass that will not delete intermediary results, and so you will be able to call .backward() again. All but the last call to backward should have the retain_graph=True option.\n",
    "Also, the gradient is accumulated, it needs to be cleared each time if you do not want accumulated gradients. \n",
    "\n",
    "In the example above, ```dz/dx = dz/dy * dy/dx = 6y * 2 = 12y```, since ```y = 5```, ```dz/dx = 60```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable containing:\n",
      " 60\n",
      "[torch.FloatTensor of size 1x1]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "z.backward()\n",
    "#if x.grad is not None:\n",
    "#    x.grad.data.zero_()\n",
    "#z.backward(retain_graph=True)\n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For multiple dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable containing:\n",
      " 75  75\n",
      " 75  75\n",
      "[torch.FloatTensor of size 2x2]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "x = Variable(torch.ones(2,2), requires_grad=True)\n",
    "y = 2 * x + 3\n",
    "z = y * y * 3\n",
    "print(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable containing:\n",
      " 60  60\n",
      " 60  60\n",
      "[torch.FloatTensor of size 2x2]\n",
      "\n",
      "Variable containing:\n",
      "  0  60\n",
      " 60   0\n",
      "[torch.FloatTensor of size 2x2]\n",
      "\n",
      "Variable containing:\n",
      "  0  60\n",
      "  0  60\n",
      "[torch.FloatTensor of size 2x2]\n",
      "\n",
      "Variable containing:\n",
      "  0   0\n",
      " 60  60\n",
      "[torch.FloatTensor of size 2x2]\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\n",
       " 0  0\n",
       " 0  0\n",
       "[torch.FloatTensor of size 2x2]"
      ]
     },
     "execution_count": 241,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z.backward(torch.ones(2,2), retain_graph=True)\n",
    "print(x.grad)\n",
    "x.grad.data.zero_()\n",
    "z.backward(torch.Tensor([[0,1],[1,0]]), retain_graph=True)\n",
    "print(x.grad)\n",
    "x.grad.data.zero_()\n",
    "z.backward(torch.Tensor([0,1]), retain_graph=True)\n",
    "print(x.grad)\n",
    "x.grad.data.zero_()\n",
    "z.backward(torch.Tensor([[0],[1]]), retain_graph=True)\n",
    "print(x.grad)\n",
    "x.grad.data.zero_()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Pytorch Kernel",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
