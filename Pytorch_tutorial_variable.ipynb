{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CS535 Pytorch Tutorial - Variable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Variable\n",
    "--------\n",
    "\n",
    "``autograd.Variable`` is the central class of the package. It wraps a\n",
    "Tensor, and supports nearly all of operations defined on it. Once you\n",
    "finish your computation you can call ``.backward()`` and have all the\n",
    "gradients computed automatically.\n",
    "\n",
    "You can access the raw tensor through the ``.data`` attribute, while the\n",
    "gradient w.r.t. this variable is accumulated into ``.grad``.\n",
    "\n",
    ".. figure:: /_static/img/Variable.png\n",
    "   :alt: Variable\n",
    "\n",
    "   Variable\n",
    "\n",
    "There’s one more class which is very important for autograd\n",
    "implementation - a ``Function``.\n",
    "\n",
    "``Variable`` and ``Function`` are interconnected and build up an acyclic\n",
    "graph, that encodes a complete history of computation. Each variable has\n",
    "a ``.grad_fn`` attribute that references a ``Function`` that has created\n",
    "the ``Variable`` (except for Variables created by the user - their\n",
    "``grad_fn is None``).\n",
    "\n",
    "If you want to compute the derivatives, you can call ``.backward()`` on\n",
    "a ``Variable``. If ``Variable`` is a scalar (i.e. it holds a one element\n",
    "data), you don’t need to specify any arguments to ``backward()``,\n",
    "however if it has more elements, you need to specify a ``grad_output``\n",
    "argument that is a tensor of matching shape.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a varible:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x = Variable(torch.ones(1,1), requires_grad=True)\n",
    "print(x)\n",
    "print(x.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "y = 2 * x + 3\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "z = y * y * 3\n",
    "\n",
    "print(z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To reduce memory usage, during the .backward() call, all the intermediary results are deleted when they are not needed anymore. Hence if you try to call .backward() again, the intermediary results don’t exist and the backward pass cannot be performed (and you get the error you see).\n",
    "You can call .backward(retain_graph=True) to make a backward pass that will not delete intermediary results, and so you will be able to call .backward() again. All but the last call to backward should have the retain_graph=True option.\n",
    "Also, the gradient is accumulated, it needs to be cleared each time if you do not want accumulated gradients. \n",
    "\n",
    "In the example above, ```dz/dx = dz/dy * dy/dx = 6y * 2 = 12y```, since ```y = 5```, ```dz/dx = 60```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#z.backward()\n",
    "if x.grad is not None:\n",
    "    x.grad.data.zero_()\n",
    "z.backward(retain_graph=True)\n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For multiple dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x = Variable(torch.ones(2,2), requires_grad=True)\n",
    "y = 2 * x + 3\n",
    "z = y * y * 3\n",
    "print(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "z.backward(torch.ones(2,2), retain_graph=True)\n",
    "print(x.grad)\n",
    "x.grad.data.zero_()\n",
    "z.backward(torch.Tensor([[0,1],[1,0]]), retain_graph=True)\n",
    "print(x.grad)\n",
    "x.grad.data.zero_()\n",
    "z.backward(torch.Tensor([0,1]), retain_graph=True)\n",
    "print(x.grad)\n",
    "x.grad.data.zero_()\n",
    "z.backward(torch.Tensor([[0],[1]]), retain_graph=True)\n",
    "print(x.grad)\n",
    "x.grad.data.zero_()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Pytorch Kernel",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
